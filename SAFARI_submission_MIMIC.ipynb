{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from utils import utils\n",
    "from utils.readers import InHospitalMortalityReader\n",
    "from utils.preprocessing import Discretizer, Normalizer\n",
    "from utils import metrics\n",
    "from utils import common_utils\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "\n",
    "small_part = False\n",
    "arg_timestep = 1.0\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build readers, discretizers, normalizers\n",
    "train_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'train'),\n",
    "                                         listfile=os.path.join(data_path, 'train_listfile.csv'),\n",
    "                                         period_length=48.0)\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'train'),\n",
    "                                       listfile=os.path.join(data_path, 'val_listfile.csv'),\n",
    "                                       period_length=48.0)\n",
    "\n",
    "discretizer = Discretizer(timestep=arg_timestep,\n",
    "                          store_masks=True,\n",
    "                          impute_strategy='previous',\n",
    "                          start_time='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer_header = discretizer.transform(train_reader.read_example(0)[\"X\"])[1].split(',')\n",
    "cont_channels = [i for (i, x) in enumerate(discretizer_header) if x.find(\"->\") == -1]\n",
    "\n",
    "normalizer = Normalizer(fields=cont_channels)  # choose here which columns to standardize\n",
    "normalizer_state = 'ihm_normalizer'\n",
    "normalizer_state = os.path.join(os.path.dirname(data_path), normalizer_state)\n",
    "normalizer.load_params(normalizer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14681, 48, 76)\n",
      "(3222, 48, 76)\n"
     ]
    }
   ],
   "source": [
    "n_trained_chunks = 0\n",
    "train_raw = utils.load_data(train_reader, discretizer, normalizer, small_part, return_names=True)\n",
    "val_raw = utils.load_data(val_reader, discretizer, normalizer, small_part, return_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '1', '3', '0', '2']\n",
      "['2', '1']\n"
     ]
    }
   ],
   "source": [
    "demographic_data = []\n",
    "diagnosis_data = []\n",
    "idx_list = []\n",
    "ethnicity_types = []\n",
    "gender_types = []\n",
    "\n",
    "demo_path = data_path + 'demographic/'\n",
    "for cur_name in os.listdir(demo_path):\n",
    "    cur_id, cur_episode = cur_name.split('_', 1)\n",
    "    cur_episode = cur_episode[:-4]\n",
    "    cur_file = demo_path + cur_name\n",
    "\n",
    "    with open(cur_file, \"r\") as tsfile:\n",
    "        header = tsfile.readline().strip().split(',')\n",
    "        if header[0] != \"Icustay\":\n",
    "            continue\n",
    "        # print(header)\n",
    "        cur_data = tsfile.readline().strip().split(',')\n",
    "        \n",
    "    if len(cur_data) == 1:\n",
    "        cur_demo = np.zeros(10)\n",
    "        cur_diag = np.zeros(128)\n",
    "    else:\n",
    "        if cur_data[1] not in ethnicity_types:\n",
    "            ethnicity_types.append(cur_data[1])\n",
    "        if cur_data[2] not in gender_types:\n",
    "            gender_types.append(cur_data[2])\n",
    "        if cur_data[3] == '':\n",
    "            cur_data[3] = 60.0               # age\n",
    "        if cur_data[4] == '':\n",
    "            cur_data[4] = 160                # height\n",
    "        if cur_data[5] == '':\n",
    "            cur_data[5] = 60                 # weight\n",
    "\n",
    "        cur_demo = np.zeros(10)\n",
    "        cur_demo[int(cur_data[1])] = 1           #ethnicity -- 0-4 ('0','1','2','3','4') \n",
    "        cur_demo[5 + int(cur_data[2]) - 1] = 1   #gender    -- 5-6 ('1','2')\n",
    "        cur_demo[7:] = cur_data[3:6]             #7-9: age/height/weight\n",
    "        cur_diag = np.array(cur_data[8:], dtype=np.int)\n",
    "\n",
    "    demographic_data.append(cur_demo)\n",
    "    diagnosis_data.append(cur_diag)\n",
    "    idx_list.append(cur_id+'_'+cur_episode)\n",
    "\n",
    "print(ethnicity_types)\n",
    "print(gender_types)\n",
    "\n",
    "for each_idx in range(7,10):\n",
    "    cur_val = []\n",
    "    for i in range(len(demographic_data)):\n",
    "        cur_val.append(demographic_data[i][each_idx])\n",
    "    cur_val = np.array(cur_val)\n",
    "    _mean = np.mean(cur_val)\n",
    "    _std = np.std(cur_val)\n",
    "    _std = _std if _std > 1e-7 else 1e-7\n",
    "    for i in range(len(demographic_data)):\n",
    "        demographic_data[i][each_idx] = (demographic_data[i][each_idx] - _mean) / _std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() == True else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "print(\"available device: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y, name):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.name = name\n",
    "\n",
    "    def __getitem__(self, index):#返回的是tensor\n",
    "        return self.x[index], self.y[index], self.name[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "train_dataset = Dataset(train_raw['data'][0], train_raw['data'][1], train_raw['names'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = Dataset(val_raw['data'][0], val_raw['data'][1], val_raw['names'])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(y_pred, y_true):\n",
    "    loss = torch.nn.BCELoss()\n",
    "    return loss(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalAttentionQKV(nn.Module):\n",
    "    def __init__(self, attention_input_dim, attention_hidden_dim, attention_type='add', dropout=None):\n",
    "        super(FinalAttentionQKV, self).__init__()\n",
    "        \n",
    "        self.attention_type = attention_type\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.attention_input_dim = attention_input_dim\n",
    "\n",
    "\n",
    "        self.W_q = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_k = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "        self.W_v = nn.Linear(attention_input_dim, attention_hidden_dim)\n",
    "\n",
    "        self.W_out = nn.Linear(attention_hidden_dim, 1)\n",
    "\n",
    "        self.b_in = nn.Parameter(torch.zeros(1,))\n",
    "        self.b_out = nn.Parameter(torch.zeros(1,))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.W_q.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_k.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_v.weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.W_out.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.Wh = nn.Parameter(torch.randn(2*attention_input_dim, attention_hidden_dim))\n",
    "        self.Wa = nn.Parameter(torch.randn(attention_hidden_dim, 1))\n",
    "        self.ba = nn.Parameter(torch.zeros(1,))\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.Wh, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.Wa, a=math.sqrt(5))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    " \n",
    "        batch_size, time_step, input_dim = input.size() # batch_size * input_dim + 1 * hidden_dim(i)\n",
    "        input_q = self.W_q(input[:,-1,:]) # b h\n",
    "        input_k = self.W_k(input)# b t h\n",
    "        input_v = self.W_v(input)# b t h\n",
    "\n",
    "        if self.attention_type == 'add': #B*T*I  @ H*I\n",
    "\n",
    "            q = torch.reshape(input_q, (batch_size, 1, self.attention_hidden_dim)) #B*1*H\n",
    "            h = q + input_k + self.b_in # b t h\n",
    "            h = self.tanh(h) #B*T*H\n",
    "            e = self.W_out(h) # b t 1\n",
    "            e = torch.reshape(e, (batch_size, time_step))# b t\n",
    "\n",
    "        elif self.attention_type == 'mul':\n",
    "            q = torch.reshape(input_q, (batch_size, self.attention_hidden_dim, 1)) #B*h 1\n",
    "            e = torch.matmul(input_k, q).squeeze(-1)#b t\n",
    "            \n",
    "        elif self.attention_type == 'concat':\n",
    "            q = input_q.unsqueeze(1).repeat(1,time_step,1)# b t h\n",
    "            k = input_k\n",
    "            c = torch.cat((q, k), dim=-1) #B*T*2I\n",
    "            h = torch.matmul(c, self.Wh)\n",
    "            h = self.tanh(h)\n",
    "            e = torch.matmul(h, self.Wa) + self.ba #B*T*1\n",
    "            e = torch.reshape(e, (batch_size, time_step)) # b t \n",
    "        \n",
    "        a = self.softmax(e) #B*T\n",
    "        if self.dropout is not None:\n",
    "            a = self.dropout(a)\n",
    "        v = torch.matmul(a.unsqueeze(1), input_v).squeeze() #B*I\n",
    "\n",
    "        return v, a\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "\n",
    "class SAFARI(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_clu, output_dim, keep_prob=0.5):\n",
    "        super(SAFARI, self).__init__()\n",
    "\n",
    "        # hyperparameters\n",
    "        self.input_dim = input_dim  \n",
    "        self.hidden_dim = hidden_dim  # d_model\n",
    "        self.output_dim = output_dim\n",
    "        self.keep_prob = keep_prob\n",
    "        self.n_clu = n_clu\n",
    "        self.dim_list = [2, 1, 1, 8, 12, 13, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "        \n",
    "        self.GRUs = nn.ModuleList()\n",
    "        for i in self.dim_list:\n",
    "            self.GRUs.append(nn.GRU(i+1, self.hidden_dim, batch_first=True))\n",
    "        self.feature_proj = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        \n",
    "        self.FinalAttentionQKV = FinalAttentionQKV(self.hidden_dim, self.hidden_dim, attention_type='mul',dropout = 1 - self.keep_prob)\n",
    "\n",
    "        self.GCN_W1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.GCN_W2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "#         self.demo_proj_main = nn.Linear(12, self.hidden_dim)\n",
    "        self.demo_proj = nn.Linear(10, self.hidden_dim)\n",
    "        self.output0 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.output1 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p = 1 - self.keep_prob)\n",
    "        self.tanh=nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu=nn.ReLU()\n",
    "        self.elu=nn.ELU()\n",
    "\n",
    "    def forward(self, input, static, adj_mat):\n",
    "        \n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = len(self.dim_list)\n",
    "        \n",
    "        assert(feature_dim == self.input_dim)# input Tensor : 256 * 48 * 76\n",
    "   \n",
    "        GRU_embeded_input = None\n",
    "\n",
    "        # MIMIC features include multi-dimensional one-hot variables, and need to separate them into different channels of encoders\n",
    "        start_pos = 0\n",
    "        for i in range(feature_dim):\n",
    "            mask_pos = 59 + i\n",
    "            tmp_input = torch.cat((input[:,:,start_pos:start_pos+self.dim_list[i]], input[:,:,mask_pos].unsqueeze(-1)), dim=-1)\n",
    "            start_pos += self.dim_list[i]\n",
    "            hidden_0 = Variable(torch.zeros(batch_size, self.hidden_dim).unsqueeze(0)).to(device)\n",
    "            embeded_input = self.GRUs[i](tmp_input, hidden_0)[1].squeeze().unsqueeze(1)\n",
    "            embeded_input = self.feature_proj(embeded_input)\n",
    "            if GRU_embeded_input is None:\n",
    "                GRU_embeded_input = embeded_input\n",
    "            else:\n",
    "                GRU_embeded_input = torch.cat((GRU_embeded_input, embeded_input), 1)\n",
    "\n",
    "        static_emb = self.feature_proj(self.relu(self.demo_proj(static))).unsqueeze(1)\n",
    "        GRU_embeded_input = torch.cat((GRU_embeded_input, static_emb), dim=1)\n",
    "        posi_input = self.dropout(GRU_embeded_input) # batch_size * d_input * hidden_dim\n",
    "\n",
    "        contexts = posi_input\n",
    "        \n",
    "        clu_context = None\n",
    "        gcn_hidden = None\n",
    "        gcn_contexts = None\n",
    "        #Graph Conv\n",
    "        if gcn_hidden is None:\n",
    "            gcn_hidden = self.relu(self.GCN_W1(torch.matmul(adj_mat, contexts)))\n",
    "        if gcn_contexts is None:\n",
    "            gcn_contexts = self.relu(self.GCN_W2(torch.matmul(adj_mat, gcn_hidden)))\n",
    "        \n",
    "\n",
    "        clu_context = gcn_contexts[:,:,:]\n",
    "\n",
    "        \n",
    "        weighted_contexts = self.FinalAttentionQKV(clu_context)[0]\n",
    "        output = self.relu(self.output0(self.dropout(weighted_contexts)))\n",
    "        output = self.output1(self.dropout(output))# b 1\n",
    "        output = self.sigmoid(output)\n",
    "#         print(weighted_contexts.shape)\n",
    "          \n",
    "        return output, weighted_contexts, GRU_embeded_input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Relational Graph Update, returns a adjacency matrix and Clustering Info\n",
    "def GraphUpdate(sim_metric, feature_emb, input_dim, n_clu, feat2clu=None):\n",
    "    adj_mat = torch.zeros(input_dim+1, input_dim+1).to(device)\n",
    "    eps = 1e-7\n",
    "    #print(feature_emb.size())\n",
    "\n",
    "    if sim_metric == 'euclidean':\n",
    "        feature_mean_emb = [None for i in range(input_dim)]\n",
    "        for i in range(input_dim):\n",
    "            feature_mean_emb[i] = torch.mean(feature_emb[:,i,:].squeeze(), dim=0).cpu().numpy()\n",
    "        feature_mean_emb = np.array(feature_mean_emb)\n",
    "        #print(feature_mean_emb.shape)\n",
    "        \n",
    "        if feat2clu is None:\n",
    "            kmeans = KMeans(n_clusters=n_clu, init='random', n_init=2).fit(feature_mean_emb)\n",
    "            feat2clu = kmeans.labels_\n",
    "        \n",
    "        clu2feat = [[] for i in range(n_clu)]\n",
    "        for i in range(input_dim):\n",
    "            clu2feat[feat2clu[i]].append(i)\n",
    "\n",
    "        for clu_id, cur_clu in enumerate(clu2feat):\n",
    "            for i in cur_clu:\n",
    "                for j in cur_clu:\n",
    "                    if i != j:\n",
    "                        cos_sim = np.dot(feature_mean_emb[i], feature_mean_emb[j])\n",
    "                        cos_sim = cos_sim / max(eps, float(np.linalg.norm(feature_mean_emb[i]) * np.linalg.norm(feature_mean_emb[j])))\n",
    "                        adj_mat[i][j] = torch.tensor(cos_sim).to(device)\n",
    "\n",
    "\n",
    "    elif 'kernel' in sim_metric:\n",
    "        kernel_mat = torch.zeros((input_dim, input_dim)).to(device)\n",
    "        sigma = 0\n",
    "        for i in range(input_dim):\n",
    "            for j in range(input_dim):\n",
    "                if sim_metric == 'rbf_kernel':\n",
    "                    sample_dist = F.pairwise_distance(feature_emb[:,i,:], feature_emb[:,j,:], p=2)\n",
    "                if sim_metric == 'laplacian_kernel':\n",
    "                    sample_dist = F.pairwise_distance(feature_emb[:,i,:], feature_emb[:,j,:], p=1)\n",
    "                sigma += torch.mean(sample_dist)\n",
    "        \n",
    "        sigma = sigma / (input_dim * input_dim)\n",
    "        #sigma = feature_emb.size(-1)\n",
    "    \n",
    "        for i in range(input_dim):\n",
    "            for j in range(input_dim):\n",
    "                if sim_metric == 'rbf_kernel':\n",
    "                    sample_dist = F.pairwise_distance(feature_emb[:,i,:], feature_emb[:,j,:], p=2)\n",
    "                    kernel_mat[i, j] = torch.mean(torch.exp(-(sample_dist * sample_dist) / (2 * (sigma**2))))\n",
    "                elif sim_metric == 'laplacian_kernel':\n",
    "                    sample_dist = F.pairwise_distance(feature_emb[:,i,:], feature_emb[:,j,:], p=1)\n",
    "                    kernel_mat[i, j] = torch.mean(torch.exp(-sample_dist / sigma))\n",
    "        #print(kernel_mat)\n",
    "        aff_mat = np.array(kernel_mat.cpu().detach().numpy())\n",
    "        #print(aff_mat)\n",
    "        \n",
    "        if feat2clu is None:\n",
    "            kmeans = SpectralClustering(n_clusters=n_clu, affinity='precomputed', n_init=20).fit(aff_mat)\n",
    "            feat2clu = kmeans.labels_\n",
    "        \n",
    "        clu2feat = [[] for i in range(n_clu)]\n",
    "        for i in range(input_dim):\n",
    "            clu2feat[feat2clu[i]].append(i)\n",
    "\n",
    "        for clu_id, cur_clu in enumerate(clu2feat):\n",
    "            for i in cur_clu:\n",
    "                for j in cur_clu:\n",
    "                    if i != j:\n",
    "                        adj_mat[i][j] = torch.tensor(aff_mat[i][j]).to(device)\n",
    "\n",
    "\n",
    "    for i in range(input_dim + 1):\n",
    "        adj_mat[i][i] = 1\n",
    "\n",
    "    for i in range(input_dim):\n",
    "        adj_mat[i][input_dim] = 1\n",
    "        adj_mat[input_dim][i] = 1\n",
    "\n",
    "    \n",
    "    return adj_mat, feat2clu, clu2feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 3407\n",
    "np.random.seed(RANDOM_SEED) #numpy\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED) # cpu\n",
    "torch.cuda.manual_seed(RANDOM_SEED) #gpu\n",
    "torch.backends.cudnn.deterministic=True # cudnn\n",
    "    \n",
    "epochs = 180\n",
    "input_dim = 17\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "n_clu = 5\n",
    "\n",
    "model = SAFARI(input_dim = input_dim, hidden_dim = hidden_dim, n_clu=n_clu, output_dim = output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50], gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = './model/MIMIC_SAFARI'\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "cluster_epochs = 0.2 * epochs\n",
    "pad_token = np.zeros(input_dim)\n",
    "max_roc = 0\n",
    "max_prc = 0\n",
    "min_loss = 999\n",
    "sim_metric = 'laplacian_kernel'\n",
    "train_loss = []\n",
    "train_model_loss = []\n",
    "valid_loss = []\n",
    "valid_model_loss = []\n",
    "history = []\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "feat2clu = np.random.randint(0, n_clu, size=input_dim)\n",
    "clu2feat = [[] for i in range(n_clu)]\n",
    "for i in range(input_dim):\n",
    "    clu2feat[feat2clu[i]].append(i)\n",
    "\n",
    "#Graph Init\n",
    "adj_mat = torch.zeros(input_dim+1, input_dim+1).to(device)\n",
    "for clu_id, cur_clu in enumerate(clu2feat):\n",
    "    for i in cur_clu:\n",
    "        for j in cur_clu:\n",
    "            if i != j:\n",
    "                adj_mat[i][j] = 1\n",
    "\n",
    "for i in range(input_dim + 1):\n",
    "    adj_mat[i][i] = 1\n",
    "    \n",
    "for i in range(input_dim):\n",
    "    adj_mat[i][input_dim] = 1\n",
    "    adj_mat[input_dim][i] = 1\n",
    "\n",
    "    \n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "    epoch_loss = []\n",
    "    print('Current cluster:', clu2feat)\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    for step, (batch_x, batch_y, batch_name) in enumerate(train_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        batch_demo = []\n",
    "        for i in range(len(batch_name)):\n",
    "            cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "            cur_idx = cur_id + '_' + cur_ep\n",
    "            cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "            batch_demo.append(cur_demo)\n",
    "        \n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "        opt, emb, contexts = model(batch_x, batch_demo, adj_mat)\n",
    "        \n",
    "        #FL_Loss = get_FL_loss(opt, batch_y.unsqueeze(-1), alpha=0.25, gamma=2)\n",
    "        FL_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "        model_loss =  100*FL_Loss\n",
    "        loss = model_loss #+ 10000 * decov_loss\n",
    "        \n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        model_batch_loss.append(model_loss.cpu().detach().numpy())\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print('Epoch %d Batch %d: Train Loss = %.4f'%(each_epoch, step, np.mean(np.array(batch_loss))))\n",
    "    train_loss.append(np.mean(np.array(batch_loss)))\n",
    "    train_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "\n",
    "\n",
    "    batch_loss = []\n",
    "    model_batch_loss = []\n",
    "    \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_x, batch_y, batch_name in valid_loader:\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "\n",
    "            batch_demo = []\n",
    "            for i in range(len(batch_name)):\n",
    "                cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "                cur_idx = cur_id + '_' + cur_ep\n",
    "                cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "                batch_demo.append(cur_demo)\n",
    "\n",
    "            batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "            opt, emb, contexts = model(batch_x, batch_demo, adj_mat)\n",
    "\n",
    "            #FL_Loss = get_FL_loss(opt, batch_y.unsqueeze(-1), alpha=0.25, gamma=2)\n",
    "            FL_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "            model_loss =  100*FL_Loss\n",
    "            loss = model_loss #+ 10000 * decov_loss\n",
    "        \n",
    "            \n",
    "            batch_loss.append(loss.cpu().detach().numpy())\n",
    "            model_batch_loss.append(model_loss.cpu().detach().numpy())\n",
    "            y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "            y_true += list(batch_y.cpu().numpy().flatten())\n",
    "            \n",
    "            \n",
    "    valid_loss.append(np.mean(np.array(batch_loss)))\n",
    "    valid_model_loss.append(np.mean(np.array(model_batch_loss)))\n",
    "    \n",
    "    print(\"\\n==>Predicting on validation\")\n",
    "    print('Valid Loss = %.4f'%(valid_loss[-1]))\n",
    "    print('valid_model Loss = %.4f'%(valid_model_loss[-1]))\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "    ret = metrics.print_metrics_binary(y_true, y_pred)\n",
    "    history.append(ret)\n",
    "    print('')\n",
    "\n",
    "    cur_prc = ret['auprc']\n",
    "    if cur_prc > max_prc:\n",
    "        max_prc = cur_prc\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': each_epoch,\n",
    "            'cluster': clu2feat,\n",
    "            'adj_mat': adj_mat\n",
    "        }\n",
    "        torch.save(state, file_name+\"prc\")\n",
    "        print('\\n------------ Save best-prc model ------------\\n')\n",
    "    \n",
    "    cur_roc = ret['auroc']\n",
    "    if cur_roc > max_roc:\n",
    "        max_roc = cur_roc\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': each_epoch,\n",
    "            'cluster': clu2feat,\n",
    "            'adj_mat': adj_mat\n",
    "        }\n",
    "        torch.save(state, file_name+\"roc\")\n",
    "        print('\\n------------ Save best-roc model ------------\\n')\n",
    "    \n",
    "    if valid_loss[-1] < min_loss:\n",
    "        min_loss = valid_loss[-1]\n",
    "        state = {\n",
    "            'net': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': each_epoch,\n",
    "            'cluster': clu2feat,\n",
    "            'adj_mat': adj_mat\n",
    "        }\n",
    "        torch.save(state, file_name+\"loss\")\n",
    "        print('\\n------------ Save best-valloss model ------------\\n')\n",
    "    \n",
    "    feature_emb = None\n",
    "    if each_epoch < cluster_epochs:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for step, (batch_x, batch_y, batch_name) in enumerate(train_loader):  \n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "\n",
    "                batch_demo = []\n",
    "                for i in range(len(batch_name)):\n",
    "                    cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "                    cur_idx = cur_id + '_' + cur_ep\n",
    "                    cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "                    batch_demo.append(cur_demo)\n",
    "\n",
    "                batch_demo = torch.stack(batch_demo).to(device)\n",
    "\n",
    "                opt, emb, contexts = model(batch_x, batch_demo, adj_mat)\n",
    "\n",
    "                if step % 30 == 0:\n",
    "                    print(\"Generating hidden from train in eval mode. Batch %d...\" % step)\n",
    "                #Sampling\n",
    "                cur_batch_size = batch_x.size(0)\n",
    "                sample_size = min(32, cur_batch_size)\n",
    "                indices = torch.tensor(random.sample(range(cur_batch_size), sample_size)).to(device)\n",
    "                if feature_emb is None:\n",
    "                    feature_emb = torch.index_select(contexts[:,:-1,:], dim=0, index=indices)\n",
    "                else:\n",
    "                    cur_feature_emb = torch.index_select(contexts[:,:-1,:], dim=0, index=indices)\n",
    "                    feature_emb = torch.cat((feature_emb, cur_feature_emb), dim=0)\n",
    "                    \n",
    "        adj_mat, feat2clu, clu2feat = GraphUpdate(sim_metric, feature_emb, input_dim, n_clu)\n",
    "\n",
    "    scheduler.step()\n",
    "        \n",
    "        \n",
    "print('==============DONE==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Plot Loss\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "x_axis = np.arange(1, 151)\n",
    "ax1.plot(x_axis, train_model_loss, c='red', label='train')\n",
    "ax1.plot(x_axis, valid_model_loss, c='blue', label='valid')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3236, 48, 76)\n"
     ]
    }
   ],
   "source": [
    "test_reader = InHospitalMortalityReader(dataset_dir=os.path.join(data_path, 'test'),\n",
    "                                            listfile=os.path.join(data_path, 'test_listfile.csv'),\n",
    "                                            period_length=48.0)\n",
    "test_raw = utils.load_data(test_reader, discretizer, normalizer, small_part, return_names=True)\n",
    "test_dataset = Dataset(test_raw['data'][0], test_raw['data'][1], test_raw['names'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 31\n",
      "Batch 0: Test Loss = 0.2869\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.2544\n",
      "confusion matrix:\n",
      "[[2803   59]\n",
      " [ 242  132]]\n",
      "accuracy = 0.9069839119911194\n",
      "precision class 0 = 0.9205254316329956\n",
      "precision class 1 = 0.6910994648933411\n",
      "recall class 0 = 0.9793850183486938\n",
      "recall class 1 = 0.3529411852359772\n",
      "AUC of ROC = 0.865084436671562\n",
      "AUC of PRC = 0.5379030136044421\n",
      "min(+P, Se) = 0.5080213903743316\n",
      "f1_score = 0.4672566288726912\n"
     ]
    }
   ],
   "source": [
    "file_name = './model/MIMIC_SAFARI'\n",
    "\n",
    "batch_size = 256\n",
    "pad_token = np.zeros(34)\n",
    "checkpoint = torch.load(file_name+'roc')#23\n",
    "save_epoch = checkpoint['epoch']\n",
    "clu2feat = checkpoint['cluster']\n",
    "adj_mat = checkpoint['adj_mat']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_name) in enumerate(test_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        batch_demo = []\n",
    "        for i in range(len(batch_name)):\n",
    "            cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "            cur_idx = cur_id + '_' + cur_ep\n",
    "            cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "            batch_demo.append(cur_demo)\n",
    "\n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "        opt, emb, contexts = model(batch_x, batch_demo, adj_mat)\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc 0.8656(0.0094)\n",
      "auprc 0.5402(0.0276)\n",
      "minpse 0.5152(0.0229)\n",
      "f1 0.4681(0.0254)\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap\n",
    "N = len(y_true)\n",
    "N_idx = np.arange(N)\n",
    "K = 1000\n",
    "\n",
    "auroc = []\n",
    "auprc = []\n",
    "minpse = []\n",
    "f1 = []\n",
    "for i in range(K):\n",
    "    boot_idx = np.random.choice(N_idx, N, replace=True)\n",
    "    boot_true = np.array(y_true)[boot_idx]\n",
    "    boot_pred = y_pred[boot_idx, :]\n",
    "    test_ret = metrics.print_metrics_binary(boot_true, boot_pred, verbose=0)\n",
    "    auroc.append(test_ret['auroc'])\n",
    "    auprc.append(test_ret['auprc'])\n",
    "    minpse.append(test_ret['minpse'])\n",
    "    f1.append(test_ret['f1_score'])\n",
    "#     print('%d/%d'%(i+1,K))\n",
    "    \n",
    "print('auroc %.4f(%.4f)'%(np.mean(auroc), np.std(auroc)))\n",
    "print('auprc %.4f(%.4f)'%(np.mean(auprc), np.std(auprc)))\n",
    "print('minpse %.4f(%.4f)'%(np.mean(minpse), np.std(minpse)))\n",
    "print('f1 %.4f(%.4f)'%(np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = [auroc, auprc, minpse, f1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim=76, hidden_dim=128, output_dim=1, dropout=0.3):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.NUM_LAYERS = 2\n",
    "        self.demo_proj = nn.Linear(10, self.hidden_dim)\n",
    "        \n",
    "        self.gru_encoder = nn.GRU(1, self.hidden_dim, self.NUM_LAYERS, batch_first=True, dropout=self.dropout)\n",
    "#         for param in self.timenet.parameters():\n",
    "#             param.requires_grad = True\n",
    "    \n",
    "        self.nn_output = nn.Linear(self.hidden_dim*(self.input_dim*2+1), self.output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.nn_dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input, static):\n",
    "        batch_size = input.size(0)\n",
    "        time_step = input.size(1)\n",
    "        feature_dim = input.size(2)\n",
    "\n",
    "        gru_atten = None\n",
    "        \n",
    "        for i in range(feature_dim):\n",
    "            \n",
    "#             tmp_input = pack_padded_sequence(input[:,:,i].unsqueeze(-1), lens, batch_first=True)\n",
    "            timenet_feature = self.gru_encoder(input[:,:,i].unsqueeze(-1))[1].transpose(0, 1)\n",
    "            timenet_feature = torch.reshape(timenet_feature, (batch_size, self.hidden_dim*2))\n",
    "            if gru_atten is None:\n",
    "                gru_atten = timenet_feature\n",
    "            else:\n",
    "                gru_atten = torch.cat((gru_atten, timenet_feature), dim=1)\n",
    "        static_emb = self.demo_proj(static)\n",
    "        gru_atten = torch.cat((gru_atten, static_emb), dim=1)\n",
    "        hn = gru_atten\n",
    "\n",
    "        if self.dropout > 0.0:\n",
    "            hn = self.nn_dropout(hn)\n",
    "\n",
    "        rnn_output = self.nn_output(hn)\n",
    "        rnn_output = self.sigmoid(rnn_output)\n",
    "\n",
    "        return rnn_output\n",
    "\n",
    "    \n",
    "input_dim = 76\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "\n",
    "model = RNN(input_dim = input_dim, hidden_dim = hidden_dim, output_dim = output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last saved model is in epoch 40\n",
      "Batch 0: Test Loss = 0.2718\n",
      "\n",
      "==>Predicting on test\n",
      "Test Loss = 0.2526\n",
      "confusion matrix:\n",
      "[[2809   53]\n",
      " [ 246  128]]\n",
      "accuracy = 0.9076019525527954\n",
      "precision class 0 = 0.9194762706756592\n",
      "precision class 1 = 0.7071823477745056\n",
      "recall class 0 = 0.9814814925193787\n",
      "recall class 1 = 0.34224599599838257\n",
      "AUC of ROC = 0.8643585316726272\n",
      "AUC of PRC = 0.5245101181936102\n",
      "min(+P, Se) = 0.506426735218509\n",
      "f1_score = 0.4612612731545188\n"
     ]
    }
   ],
   "source": [
    "file_name = './model/mimic_timenet'\n",
    "\n",
    "batch_size = 256\n",
    "checkpoint = torch.load(file_name+'roc')#23\n",
    "save_epoch = checkpoint['epoch']\n",
    "print(\"last saved model is in epoch {}\".format(save_epoch))\n",
    "model.load_state_dict(checkpoint['net'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "model.eval()\n",
    "\n",
    "\n",
    "batch_loss = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for step, (batch_x, batch_y, batch_name) in enumerate(test_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        batch_demo = []\n",
    "        for i in range(len(batch_name)):\n",
    "            cur_id, cur_ep, _ = batch_name[i].split('_', 2)\n",
    "            cur_idx = cur_id + '_' + cur_ep\n",
    "            cur_demo = torch.tensor(demographic_data[idx_list.index(cur_idx)], dtype=torch.float32)\n",
    "            batch_demo.append(cur_demo)\n",
    "\n",
    "        batch_demo = torch.stack(batch_demo).to(device)\n",
    "        opt = model(batch_x, batch_demo)\n",
    "        BCE_Loss = get_loss(opt, batch_y.unsqueeze(-1))\n",
    "        model_loss =  BCE_Loss \n",
    "\n",
    "        loss = model_loss\n",
    "        batch_loss.append(loss.cpu().detach().numpy())\n",
    "        if step % 20 == 0:\n",
    "            print('Batch %d: Test Loss = %.4f'%(step, loss.cpu().detach().numpy()))\n",
    "        y_pred += list(opt.cpu().detach().numpy().flatten())\n",
    "        y_true += list(batch_y.cpu().numpy().flatten())\n",
    "\n",
    "print(\"\\n==>Predicting on test\")\n",
    "print('Test Loss = %.4f'%(np.mean(np.array(batch_loss))))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.stack([1 - y_pred, y_pred], axis=1)\n",
    "test_res = metrics.print_metrics_binary(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc 0.8642(0.0093)\n",
      "auprc 0.5245(0.0273)\n",
      "minpse 0.5083(0.0224)\n",
      "f1 0.4609(0.0253)\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap\n",
    "N = len(y_true)\n",
    "N_idx = np.arange(N)\n",
    "K = 1000\n",
    "\n",
    "cmp_auroc = []\n",
    "cmp_auprc = []\n",
    "cmp_minpse = []\n",
    "cmp_f1 = []\n",
    "for i in range(K):\n",
    "    boot_idx = np.random.choice(N_idx, N, replace=True)\n",
    "    boot_true = np.array(y_true)[boot_idx]\n",
    "    boot_pred = y_pred[boot_idx, :]\n",
    "    test_ret = metrics.print_metrics_binary(boot_true, boot_pred, verbose=0)\n",
    "    cmp_auroc.append(test_ret['auroc'])\n",
    "    cmp_auprc.append(test_ret['auprc'])\n",
    "    cmp_minpse.append(test_ret['minpse'])\n",
    "    cmp_f1.append(test_ret['f1_score'])\n",
    "#     print('%d/%d'%(i+1,K))\n",
    "\n",
    "cmp_all = [cmp_auroc, cmp_auprc, cmp_minpse, cmp_f1]\n",
    "print('auroc %.4f(%.4f)'%(np.mean(cmp_auroc), np.std(cmp_auroc)))\n",
    "print('auprc %.4f(%.4f)'%(np.mean(cmp_auprc), np.std(cmp_auprc)))\n",
    "print('minpse %.4f(%.4f)'%(np.mean(cmp_minpse), np.std(cmp_minpse)))\n",
    "print('f1 %.4f(%.4f)'%(np.mean(cmp_f1), np.std(cmp_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------------+-----------------------+---------+--------+--------+\n",
      "| feature |   t   | Mean in SAFARI | Mean in Best Baseline | p-value | p<0.05 | p<0.01 |\n",
      "+---------+-------+----------------+-----------------------+---------+--------+--------+\n",
      "|  AUROC  | 3.276 |     0.866      |         0.864         |  0.001  |   Y    |   Y    |\n",
      "|  AUPRC  | 12.28 |      0.54      |         0.524         |   0.0   |   Y    |   Y    |\n",
      "|  minPSE | 6.518 |     0.515      |         0.508         |   0.0   |   Y    |   Y    |\n",
      "+---------+-------+----------------+-----------------------+---------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "from scipy.stats import ranksums\n",
    "table = PrettyTable(['feature','t','Mean in SAFARI','Mean in Best Baseline','p-value','p<0.05','p<0.01'])\n",
    "\n",
    "col_name = ['AUROC', 'AUPRC', 'minPSE']\n",
    "for idx in range(3):\n",
    "    posi_li = list(all[idx])\n",
    "    nega_li = list(cmp_all[idx])\n",
    "    \n",
    "    \n",
    "    total_li = []\n",
    "    total_li.extend(posi_li)\n",
    "    total_li.extend(nega_li)\n",
    "    total_li.sort()\n",
    "#     print(total_li)\n",
    "    s_cut = total_li[int(len(total_li)*0.05)]\n",
    "    b_cut = total_li[int(len(total_li)*0.95)]\n",
    "#     print(s_cut)\n",
    "    t,p = ranksums(posi_li, nega_li)\n",
    "#     t2,p = mannwhitneyu(nega_li, posi_li)\n",
    "    flag = \"\"\n",
    "    if p < 0.05:\n",
    "        flag = \"Y\"\n",
    "    flag2 = \"\"\n",
    "    if p < 0.01:\n",
    "        flag2 = \"Y\"\n",
    "        \n",
    "#     flag3 = \"\"\n",
    "#     if p < 0.1:\n",
    "#         flag3 = \"Y\"\n",
    "        \n",
    "    rela1 = ''\n",
    "    rela2 = ''\n",
    "    if t > 0:\n",
    "        rela1 = \"Y\"\n",
    "    else:\n",
    "        rela2 = \"Y\"\n",
    "    \n",
    "    table.add_row([col_name[idx],round(t,3), round(np.mean(posi_li),3),round(np.mean(nega_li),3),round(p,3),flag,flag2])\n",
    "#     break\n",
    "# table.align[\"feature\"] = \"r\"  \n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
